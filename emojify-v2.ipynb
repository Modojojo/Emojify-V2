{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji #to show emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"DATA\\\\train.csv\" \n",
    "test_dir = \"DATA\\\\test.csv\"\n",
    "emb_path = \"EMBEDDINGS\\\\glove.6B.50d.txt\" # glove file path (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns :  Index(['sentence', 'emoji_id'], dtype='object') \n",
      "\n",
      "                          sentence  emoji_id\n",
      "0           never talk to me again         3\n",
      "1  I am proud of your achievements         2\n",
      "2   It is the worst day in my life         3\n",
      "3                 Miss you so much         0\n",
      "4                     food is life         4 \n",
      "\n",
      "Number of classes :  5 \n",
      "\n",
      "Class Distribution : \n",
      "0    0.166667\n",
      "1    0.143939\n",
      "2    0.287879\n",
      "3    0.272727\n",
      "4    0.128788\n",
      "Name: emoji_id, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_dir)\n",
    "test_data = pd.read_csv(test_dir)\n",
    "print(\"Columns : \", train_data.columns, \"\\n\")\n",
    "print(train_data.head(), '\\n')\n",
    "print(\"Number of classes : \",len(set(train_data[\"emoji_id\"])), \"\\n\")\n",
    "print(\"Class Distribution : \\n\" + str(train_data[\"emoji_id\"].value_counts(normalize = True, sort = False)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining a dictionary for emojis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’™ âš¾ ğŸ˜ƒ ğŸ˜« ğŸ´ "
     ]
    }
   ],
   "source": [
    "emoji_dict ={0:\":blue_heart:\",\n",
    "             1:\":baseball:\",\n",
    "             2:\":grinning_face_with_big_eyes:\",\n",
    "             3:\":tired_face:\",\n",
    "             4:\":fork_and_knife:\"}\n",
    "\n",
    "for i in range(len(emoji_dict)):\n",
    "    print(emoji.emojize(emoji_dict[i]), end =\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the GloVe word vector embedding file : **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The function loads the GloVe (word vector) file as a dictionary which contains 400000 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the glove file\n",
    "\n",
    "def load_embeddings(gloveFile):\n",
    "    print(\"Loading Embeddings..........\",end=\"\")\n",
    "    f = open(gloveFile,  \"r\", encoding=\"utf8\")\n",
    "    emb = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        emb[word] = embedding\n",
    "    print(\"**Done**.\")\n",
    "    print(\"Total words loaded : \", len(emb))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings..........**Done**.\n",
      "Total words loaded :  400000\n"
     ]
    }
   ],
   "source": [
    "emb = load_embeddings(emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINING DICTIONARIES TO STORE WORDS AND INDEX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index ,index_to_word = {}, {} \n",
    "index = 0\n",
    "for i in emb:\n",
    "    word_to_index[i] = index\n",
    "    index_to_word[index] = i\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LOADING DATA** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a pipeline to load data, preprocess and convert it to keraas compatible form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitted_data(data):\n",
    "    \"\"\"Converts sentences into list of words. returns list of list of words.\"\"\"\n",
    "    splitted = []\n",
    "    for line in data:\n",
    "        words = line.lower().split()\n",
    "        splitted.append(words)\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equal_len_sentences(data, max_len):\n",
    "    \"\"\"Converts sentences into equal lengths by padding with 0.\"\"\"\n",
    "    n = len(data)\n",
    "    X_indices = np.zeros((n, max_len))\n",
    "    for i in range(n):\n",
    "        wordlist = data[i]\n",
    "        j = 0 \n",
    "        for word in wordlist:\n",
    "            X_indices[i,j] = word_to_index[word]\n",
    "            j += 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(traindata, testdata, trainlabels, testlabels):\n",
    "    \"\"\"Converts data into keras compatible form, returns xtrain, xtest, ytrain, ytest, max_len (length of longest sentence).\"\"\"\n",
    "    \n",
    "    split_train = get_splitted_data(traindata) #splitting sentences into list of words. \n",
    "    split_test = get_splitted_data(testdata)\n",
    "    \n",
    "    max_len = len(max(split_train, key=len)) # finding the length of the longest sentence.\n",
    "    \n",
    "    indiced_train = get_equal_len_sentences(split_train, max_len)\n",
    "    indiced_test = get_equal_len_sentences(split_test, max_len)\n",
    "    \n",
    "    xtrain, ytrain = np.array(indiced_train), keras.utils.to_categorical(trainlabels)\n",
    "    \n",
    "    xtest, ytest = np.array(indiced_test), keras.utils.to_categorical(testlabels)\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data from pandas dataframe to lists of data and labels.\n",
    "\n",
    "sentence_train = train_data[\"sentence\"].values.tolist()\n",
    "id_train = train_data[\"emoji_id\"].values.tolist()\n",
    "sentence_test = test_data[\"sentence\"].values.tolist()\n",
    "id_test = test_data[\" emoji_id\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using above defined pipeline to load data.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, max_len = load_data(sentence_train, sentence_test, id_train, id_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Creating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the embeddings into the keras Embedding() layer and setting trainable to False.\n",
    "\n",
    "def pretrained_embedding_layer():\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1 \n",
    "    emb_dim = emb[\"cucumber\"].shape[0] \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = emb[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model using the pre trained embedding layer and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    input_shape = (max_len,)\n",
    "    \n",
    "    inputs = Input(input_shape, dtype= \"int32\")\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    embeddings = embedding_layer(inputs)\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(embeddings)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = LSTM(256, return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(5)(x)\n",
    "    \n",
    "    x = Activation('softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x) # model instance\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,618,807\n",
      "Trainable params: 618,757\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 5s 41ms/step - loss: 1.5905 - acc: 0.2348\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.5389 - acc: 0.2500\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.5148 - acc: 0.3030\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.4818 - acc: 0.3258\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.4021 - acc: 0.4242\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.2721 - acc: 0.5076\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.2018 - acc: 0.5379\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.1103 - acc: 0.5606\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.9676 - acc: 0.6061\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.9050 - acc: 0.6061\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.8314 - acc: 0.6667\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.8119 - acc: 0.6818\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.6908 - acc: 0.7121\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.6306 - acc: 0.7273\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5052 - acc: 0.8182\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4246 - acc: 0.8561\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4172 - acc: 0.8333\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.8561\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3582 - acc: 0.8939\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4218 - acc: 0.8182\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4211 - acc: 0.8409\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - acc: 0.8788\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5250 - acc: 0.7879\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5693 - acc: 0.7803\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - acc: 0.9015\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - acc: 0.9015\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - acc: 0.9015\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2151 - acc: 0.9242\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2384 - acc: 0.9015\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1338 - acc: 0.9470\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1401 - acc: 0.9545\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1015 - acc: 0.9773\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0880 - acc: 0.9773\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1487 - acc: 0.9470\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1232 - acc: 0.9697\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0607 - acc: 0.9848\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0605 - acc: 0.9848\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0605 - acc: 0.9848\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0417 - acc: 0.9924\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0541 - acc: 0.9773\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0463 - acc: 0.9848\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5458 - acc: 0.8939\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2729 - acc: 0.9015\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3629 - acc: 0.8788\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1418 - acc: 0.9545\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1680 - acc: 0.9394\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1542 - acc: 0.9318\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0901 - acc: 0.9697\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1047 - acc: 0.9697\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0906 - acc: 0.9773\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Using callbacks - reduce learning rate on plateau.\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n",
    "model_history = model.fit(X_train, Y_train, epochs = 50, batch_size = 32, shuffle=True, verbose = 1, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy =  0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions and creating sentences using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to eat ğŸ´\n",
      "he did not answer ğŸ˜«\n",
      "he got a very nice raise ğŸ˜ƒ\n",
      "she got me a nice present ğŸ˜ƒ\n",
      "ha ha ha it was so funny  ğŸ˜ƒ\n",
      "he is a good friend ğŸ˜ƒ\n",
      "I am upset ğŸ˜«\n",
      "We had such a lovely dinner tonight ğŸ˜ƒ\n",
      "where is the food ğŸ´\n",
      "Stop making this joke ha ha ha ğŸ˜ƒ\n",
      "where is the ball âš¾\n",
      "are you serious ğŸ˜«\n",
      "Let us go play baseball\t âš¾\n",
      "This stupid grader is not working ğŸ˜«\n",
      "Congratulation for having a baby ğŸ˜ƒ\n",
      "stop pissing me off  ğŸ˜«\n",
      "I boiled rice ğŸ´\n",
      "Why are you feeling bad ğŸ˜«\n",
      "I am upset ğŸ˜«\n",
      "give me the ball  âš¾\n",
      "My grandmother is the love of my life ğŸ’™\n",
      "enjoy your game  âš¾\n",
      "valentine day is near  ğŸ˜ƒ\n",
      "I miss you so much ğŸ’™\n",
      "throw the ball âš¾\n",
      "My life is so boring ğŸ˜«\n",
      "she said yes\t  ğŸ˜ƒ\n",
      "will you be my valentine ğŸ˜ƒ\n",
      "he can pitch really well âš¾\n",
      "dance with me ğŸ˜ƒ\n",
      "I am hungry  ğŸ´\n",
      "See you at the restaurant ğŸ´\n",
      "I like to laugh ğŸ˜ƒ\n",
      "I will  run  âš¾\n",
      "I like your jacket  ğŸ’™\n",
      "i miss her ğŸ’™\n",
      "what is your favorite baseball game âš¾\n",
      "Good job ğŸ˜ƒ\n",
      "I love you to the stars and back ğŸ’™\n",
      "What you did was awesome ğŸ˜ƒ\n",
      "ha ha ha lol ğŸ˜ƒ\n",
      "I do not want to joke ğŸ˜«\n",
      "you are failing this exercise  ğŸ˜«\n",
      "Good joke ğŸ˜ƒ\n",
      "You deserve this nice prize ğŸ˜ƒ\n",
      "I did not have breakfast ğŸ´\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing sentences with correct predictions\n",
    "for i in range(len(pred)):\n",
    "    if np.argmax(pred[i]) == id_test[i]:\n",
    "        print(sentence_test[i], emoji.emojize(emoji_dict[np.argmax(pred[i])]))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some sentences that were not correctly predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: work is hard ğŸ˜ƒ ._____Correct Prediction :  work is hard ğŸ˜«\n",
      "Prediction: This girl is messing with me ğŸ’™ ._____Correct Prediction :  This girl is messing with me ğŸ˜«\n",
      "Prediction: work is horrible ğŸ˜ƒ ._____Correct Prediction :  work is horrible ğŸ˜«\n",
      "Prediction: any suggestions for dinner ğŸ˜ƒ ._____Correct Prediction :  any suggestions for dinner ğŸ´\n",
      "Prediction: I love taking breaks ğŸ˜« ._____Correct Prediction :  I love taking breaks ğŸ’™\n",
      "Prediction: you brighten my day ğŸ’™ ._____Correct Prediction :  you brighten my day ğŸ˜ƒ\n",
      "Prediction: she is a bully ğŸ’™ ._____Correct Prediction :  she is a bully ğŸ˜«\n",
      "Prediction: go away\t âš¾ ._____Correct Prediction :  go away\t ğŸ˜«\n",
      "Prediction: yesterday we lost again âš¾ ._____Correct Prediction :  yesterday we lost again ğŸ˜«\n",
      "Prediction: family is all I have ğŸ˜ƒ ._____Correct Prediction :  family is all I have ğŸ’™\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    if np.argmax(pred[i]) != id_test[i]:\n",
    "        print(\"Prediction:\",sentence_test[i], emoji.emojize(emoji_dict[np.argmax(pred[i])]), \"._____Correct Prediction : \",sentence_test[i], emoji.emojize(emoji_dict[id_test[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
