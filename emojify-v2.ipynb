{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji #to show emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"DATA\\\\train.csv\" \n",
    "test_dir = \"DATA\\\\test.csv\"\n",
    "emb_path = \"EMBEDDINGS\\\\glove.6B.50d.txt\" # glove file path (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns :  Index(['sentence', 'emoji_id'], dtype='object') \n",
      "\n",
      "                          sentence  emoji_id\n",
      "0           never talk to me again         3\n",
      "1  I am proud of your achievements         2\n",
      "2   It is the worst day in my life         3\n",
      "3                 Miss you so much         0\n",
      "4                     food is life         4 \n",
      "\n",
      "Number of classes :  5 \n",
      "\n",
      "Class Distribution : \n",
      "0    0.166667\n",
      "1    0.143939\n",
      "2    0.287879\n",
      "3    0.272727\n",
      "4    0.128788\n",
      "Name: emoji_id, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_dir)\n",
    "test_data = pd.read_csv(test_dir)\n",
    "print(\"Columns : \", train_data.columns, \"\\n\")\n",
    "print(train_data.head(), '\\n')\n",
    "print(\"Number of classes : \",len(set(train_data[\"emoji_id\"])), \"\\n\")\n",
    "print(\"Class Distribution : \\n\" + str(train_data[\"emoji_id\"].value_counts(normalize = True, sort = False)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining a dictionary for emojis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíô ‚öæ üòÉ üò´ üç¥ "
     ]
    }
   ],
   "source": [
    "emoji_dict ={0:\":blue_heart:\",\n",
    "             1:\":baseball:\",\n",
    "             2:\":grinning_face_with_big_eyes:\",\n",
    "             3:\":tired_face:\",\n",
    "             4:\":fork_and_knife:\"}\n",
    "\n",
    "for i in range(len(emoji_dict)):\n",
    "    print(emoji.emojize(emoji_dict[i]), end =\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the GloVe word vector embedding file : **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The function loads the GloVe (word vector) file as a dictionary which contains 400000 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the glove file\n",
    "\n",
    "def load_embeddings(gloveFile):\n",
    "    print(\"Loading Embeddings..........\",end=\"\")\n",
    "    f = open(gloveFile,  \"r\", encoding=\"utf8\")\n",
    "    emb = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        emb[word] = embedding\n",
    "    print(\"**Done**.\")\n",
    "    print(\"Total words loaded : \", len(emb))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings..........**Done**.\n",
      "Total words loaded :  400000\n"
     ]
    }
   ],
   "source": [
    "emb = load_embeddings(emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINING DICTIONARIES TO STORE WORDS AND INDEX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index ,index_to_word = {}, {} \n",
    "index = 0\n",
    "for i in emb:\n",
    "    word_to_index[i] = index\n",
    "    index_to_word[index] = i\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LOADING DATA** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a pipeline to load data, preprocess and convert it to keraas compatible form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitted_data(data):\n",
    "    \"\"\"Converts sentences into list of words. returns list of list of words.\"\"\"\n",
    "    splitted = []\n",
    "    for line in data:\n",
    "        words = line.lower().split()\n",
    "        splitted.append(words)\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equal_len_sentences(data, max_len):\n",
    "    \"\"\"Converts sentences into equal lengths by padding with 0.\"\"\"\n",
    "    n = len(data)\n",
    "    X_indices = np.zeros((n, max_len))\n",
    "    for i in range(n):\n",
    "        wordlist = data[i]\n",
    "        j = 0 \n",
    "        for word in wordlist:\n",
    "            X_indices[i,j] = word_to_index[word]\n",
    "            j += 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(traindata, testdata, trainlabels, testlabels):\n",
    "    \"\"\"Converts data into keras compatible form, returns xtrain, xtest, ytrain, ytest, max_len (length of longest sentence).\"\"\"\n",
    "    \n",
    "    split_train = get_splitted_data(traindata) #splitting sentences into list of words. \n",
    "    split_test = get_splitted_data(testdata)\n",
    "    \n",
    "    max_len = len(max(split_train, key=len)) # finding the length of the longest sentence.\n",
    "    \n",
    "    indiced_train = get_equal_len_sentences(split_train, max_len)\n",
    "    indiced_test = get_equal_len_sentences(split_test, max_len)\n",
    "    \n",
    "    xtrain, ytrain = np.array(indiced_train), keras.utils.to_categorical(trainlabels)\n",
    "    \n",
    "    xtest, ytest = np.array(indiced_test), keras.utils.to_categorical(testlabels)\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data from pandas dataframe to lists of data and labels.\n",
    "\n",
    "sentence_train = train_data[\"sentence\"].values.tolist()\n",
    "id_train = train_data[\"emoji_id\"].values.tolist()\n",
    "sentence_test = test_data[\"sentence\"].values.tolist()\n",
    "id_test = test_data[\" emoji_id\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using above defined pipeline to load data.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, max_len = load_data(sentence_train, sentence_test, id_train, id_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Creating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the embeddings into the keras Embedding() layer and setting trainable to False.\n",
    "\n",
    "def pretrained_embedding_layer():\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1 \n",
    "    emb_dim = emb[\"cucumber\"].shape[0] \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = emb[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model using the pre trained embedding layer and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    input_shape = (max_len,)\n",
    "    \n",
    "    inputs = Input(input_shape, dtype= \"int32\")\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    embeddings = embedding_layer(inputs)\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(embeddings)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = LSTM(256, return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(5)(x)\n",
    "    \n",
    "    x = Activation('softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x) # model instance\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,618,807\n",
      "Trainable params: 618,757\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 5s 41ms/step - loss: 1.5905 - acc: 0.2348\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.5389 - acc: 0.2500\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.5148 - acc: 0.3030\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.4818 - acc: 0.3258\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.4021 - acc: 0.4242\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.2721 - acc: 0.5076\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.2018 - acc: 0.5379\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.1103 - acc: 0.5606\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.9676 - acc: 0.6061\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.9050 - acc: 0.6061\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.8314 - acc: 0.6667\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.8119 - acc: 0.6818\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.6908 - acc: 0.7121\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.6306 - acc: 0.7273\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5052 - acc: 0.8182\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4246 - acc: 0.8561\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4172 - acc: 0.8333\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.8561\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3582 - acc: 0.8939\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4218 - acc: 0.8182\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4211 - acc: 0.8409\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - acc: 0.8788\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5250 - acc: 0.7879\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5693 - acc: 0.7803\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - acc: 0.9015\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - acc: 0.9015\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - acc: 0.9015\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2151 - acc: 0.9242\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2384 - acc: 0.9015\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1338 - acc: 0.9470\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1401 - acc: 0.9545\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1015 - acc: 0.9773\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0880 - acc: 0.9773\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1487 - acc: 0.9470\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1232 - acc: 0.9697\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0607 - acc: 0.9848\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0605 - acc: 0.9848\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0605 - acc: 0.9848\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0417 - acc: 0.9924\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0541 - acc: 0.9773\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0463 - acc: 0.9848\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.5458 - acc: 0.8939\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2729 - acc: 0.9015\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3629 - acc: 0.8788\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1418 - acc: 0.9545\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1680 - acc: 0.9394\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1542 - acc: 0.9318\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0901 - acc: 0.9697\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.1047 - acc: 0.9697\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.0906 - acc: 0.9773\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Using callbacks - reduce learning rate on plateau.\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n",
    "model_history = model.fit(X_train, Y_train, epochs = 50, batch_size = 32, shuffle=True, verbose = 1, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy =  0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions and creating sentences using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to eat üç¥\n",
      "he did not answer üò´\n",
      "he got a very nice raise üòÉ\n",
      "she got me a nice present üòÉ\n",
      "ha ha ha it was so funny  üòÉ\n",
      "he is a good friend üòÉ\n",
      "I am upset üò´\n",
      "We had such a lovely dinner tonight üòÉ\n",
      "where is the food üç¥\n",
      "Stop making this joke ha ha ha üòÉ\n",
      "where is the ball ‚öæ\n",
      "are you serious üò´\n",
      "Let us go play baseball\t ‚öæ\n",
      "This stupid grader is not working üò´\n",
      "Congratulation for having a baby üòÉ\n",
      "stop pissing me off  üò´\n",
      "I boiled rice üç¥\n",
      "Why are you feeling bad üò´\n",
      "I am upset üò´\n",
      "give me the ball  ‚öæ\n",
      "My grandmother is the love of my life üíô\n",
      "enjoy your game  ‚öæ\n",
      "valentine day is near  üòÉ\n",
      "I miss you so much üíô\n",
      "throw the ball ‚öæ\n",
      "My life is so boring üò´\n",
      "she said yes\t  üòÉ\n",
      "will you be my valentine üòÉ\n",
      "he can pitch really well ‚öæ\n",
      "dance with me üòÉ\n",
      "I am hungry  üç¥\n",
      "See you at the restaurant üç¥\n",
      "I like to laugh üòÉ\n",
      "I will  run  ‚öæ\n",
      "I like your jacket  üíô\n",
      "i miss her üíô\n",
      "what is your favorite baseball game ‚öæ\n",
      "Good job üòÉ\n",
      "I love you to the stars and back üíô\n",
      "What you did was awesome üòÉ\n",
      "ha ha ha lol üòÉ\n",
      "I do not want to joke üò´\n",
      "you are failing this exercise  üò´\n",
      "Good joke üòÉ\n",
      "You deserve this nice prize üòÉ\n",
      "I did not have breakfast üç¥\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing sentences with correct predictions\n",
    "for i in range(len(pred)):\n",
    "    if np.argmax(pred[i]) == id_test[i]:\n",
    "        print(sentence_test[i], emoji.emojize(emoji_dict[np.argmax(pred[i])]))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some sentences that were not correctly predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: work is hard üòÉ ._____Correct Prediction :  work is hard üò´\n",
      "Prediction: This girl is messing with me üíô ._____Correct Prediction :  This girl is messing with me üò´\n",
      "Prediction: work is horrible üòÉ ._____Correct Prediction :  work is horrible üò´\n",
      "Prediction: any suggestions for dinner üòÉ ._____Correct Prediction :  any suggestions for dinner üç¥\n",
      "Prediction: I love taking breaks üò´ ._____Correct Prediction :  I love taking breaks üíô\n",
      "Prediction: you brighten my day üíô ._____Correct Prediction :  you brighten my day üòÉ\n",
      "Prediction: she is a bully üíô ._____Correct Prediction :  she is a bully üò´\n",
      "Prediction: go away\t ‚öæ ._____Correct Prediction :  go away\t üò´\n",
      "Prediction: yesterday we lost again ‚öæ ._____Correct Prediction :  yesterday we lost again üò´\n",
      "Prediction: family is all I have üòÉ ._____Correct Prediction :  family is all I have üíô\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    if np.argmax(pred[i]) != id_test[i]:\n",
    "        print(\"Prediction:\",sentence_test[i], emoji.emojize(emoji_dict[np.argmax(pred[i])]), \"._____Correct Prediction : \",sentence_test[i], emoji.emojize(emoji_dict[id_test[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
